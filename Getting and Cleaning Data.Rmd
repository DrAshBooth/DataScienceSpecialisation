---
title: "Getting and Cleaning Data"
output: html_document
---

This document contains information and code from the coursera "Getting and Cleaning" course

# Week 2
The second week focusses on reading data from databases, the web and APIs.

## Classes

### MYSQL
MySQL is a free and open source databse software that is widely used in internet based applications. Within it, data are structured in:

* Databases
* Tables
* Fields withing tables

Each row is called a record.

### Connecting to and listing MySQL databases from R
```
library(RMySQL)
ucscDb <- dbConnect(MySQL(), user="genome",
                    host="genome-mysql.cse.ucsc.edu")
result <-dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb);
result
```

### Connecting to a database and listing the tables
```{r}
library(RMySQL)
hg19 <- dbConnect(MySQL(), user="genome", db="hg19",
                    host="genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(hg19)
length(allTables)
allTables[1:5]
```

### Getting the dimensions of a specific table
```{r}
dbListFields(hg19,"affyU133Plus2")
dbGetQuery(hg19,"select count(*) from affyU133Plus2")
```

### Reading from a table
```{r warning=FALSE}
affyData <- dbReadTable(hg19,"affyU133Plus2")
```

### Selecting a subset
Often and entire table will be far to large to read into memory and we will have to select subsets using SQL.
```{r warning=FALSE}
query <-dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <-fetch(query); quantile(affyMis$misMatches)
```

We can chose to only fetch the first n results:
```{r}
affyMisSmall <- fetch(query,n=10); dbClearResult(query);
dbDisconnect(hg19)
```

### Reading data from HDF5
HDF5 is used for storing large datasets and supports storing a nrange of data types, HDF stands for heirarcical data format and is named so as the data is stored in groups containing zero or more data sets along with their metadata. Groups contain:

* A Group header - with the group name and list of attributes.
* A group symbol table with a list of objects in the group.

Each dataset is a multimensional array of elements with their metadata including:

* A header - with a name, datatype, dataspace and storage layout.
* A data array - with the data.

### Installing the R hdf5 package
This is done through bioconductor as follows:
```
source("http://bioconductor.org/biocLite.R")
biocLite("rfdf5")
library(rhdf5)
created = h5createFile("example.h5")
created
```

### Creating groups
```
created = h5createGroup("example.h5", "foo")
created = h5createGroup("example.h5", "foo")
h5ls("example.h5"
```

### Writing to groups
```
A = matrix(1:10,nrow=5,ncol=2)
h5write(A,"example.h5", "foo/A")
```

### Writing a dataset
```
df = data.frame(1L:5L,seq(0,1,length.out=5),
                c("ab", "cde", "fghi", "a", "s"), stringsAsFactors=FALSE)
h5write(df, "example.h5", "df")
h5ls("example.h5")
```

### Reading data
```
readA = h5read("example.h5", "foo/A")
readdf = h5read("examlple.h5","df")
readA
```

### Web scraping
Web scraping involves programmatically extracting data from the html code of websites

### readlines()
```{r}
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
```

### Parsing with XML
```{r}
library(XML)
url<-"http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html<-htmlTreeParse(url, useInternalNodes = T)
xpathSApply(html,"//title", xmlValue)
xpathSApply(html,"//td[@id='col-citedby']")
```

### GET() from the httr package
```{r}
library(httr); html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2, asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
```

### Accessing websites with passwords
```{r}
pg2 = GET("http://httpbin.org/basic-auth/user/passwd",
          authenticate("user", "passwd"))
pg2
names(pg2)
```

### Using handles
Using handles allows you to save the authentication across multiple requests
```{r}
google = handle("http://google.com")
pg1 = GET(handle=google,path="/")
pg2 = GET(handle=google,path="search")
```

### APIs
Appliction programming interfaces are a great way to interact with software and the web. Often you will need to create a developer account to access them (this is the case with Twitter). 

### Accessing Twitter from R
```
myapp = oauth_app("twitter",
                  key="yourConsumerKeyHere",
                  secret="yourConsumerSecretHere")
sig = sign_oauth1.0(myapp,
                    token = "yourTokenHere",
                    token_secret = "yourTokenSecretHere")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
```

### Converting the json object
```
json = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
```


### Reading data from other programs
The foreign package is very usefull for this. Tends to revolve around the ```read.foo()``` syntax, as in:

* read.arff (Weka)
* read.dta (Stata)
* read.mtp (Minitab)
* read.octave (Octave)
* read.spss (SPSS)
* read.xport (SAS)

### Other Database types
There are R packages for a variety of databases including:

* RPostresSQL
* RODBC
* RMongo

### Reading images
You can also read in a number of image formats, including:

* jpeg
* readbitmap
* png

### Reading GIS data
There are many packages available for reading GIS data:

* rdgal
* rgeos
* raster

### Reading music data
You can also read directly from mp3 files using:

* tuneR
* seeWave

## Quiz - W2

### Question 1
We'll need the httr package for this
```
library(httr)
```

First, register an application at https://github.com/settings/applications; use any URL you would like for the homepage URL and http://localhost:1410 as the callback url. Insert your client ID and secret below - if secret is omitted, it will look it up in the ```GITHUB_CONSUMER_SECRET``` environmental variable.
```
myapp <- oauth_app("github", 
                   key="0638a2137e9e0592fa94",
                   secret = "81adc9c11643b6c98b9eae82ad69d768e07b8e5d")
```

Next, we get the OAuth credentials:
```
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
```

We can now use the API to grab the data from Jeff's github
```
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
```

### Question 2
The sqldf package allows for execution of SQL commands on R data frames. We will use the sqldf package to practice the queries we might send with the dbSendQuery command in RMySQL. Download the American Community Survey data and load it into an R object:
```
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl,destfile = "./data/acs.csv", method="curl")
acs <- read.csv("./data/acs.csv")
```

To select only the data for the probability weights pwgtp1 with ages less than 50, we can use the SQL query:
```
sqldf("select pwgtp1 from acs where AGEP < 50")
```

### Question 3
Using the same data frame from the previous problem, the equivalent function to unique(acs$AGEP) is:
```
sqldf("select distinct AGEP from acs")
```

### Question 4
How many characters are in the 10th, 20th, 30th and 100th lines of HTML from this page "http://biostat.jhsph.edu/~jleek/contact.html"?
```{r}
con = url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode = readLines(con)
close(con)
nchar(htmlCode[c(10,20,30,100)])
```

### Question 5
Read this data set into R and report the sum of the numbers in the fourth of the nine columns. 

https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for 
```{r}
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
sst <- read.fwf(file=url(fileUrl),
                skip=4,
                widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4))
sum(sst[,4])
```








